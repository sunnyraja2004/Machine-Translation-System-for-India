# CS779(Natural Language Processing) Competition Submission: Machine Translation System for India ğŸ‡®ğŸ‡³

This repository contains my submission for the **CS779(Natural Language Processing) Competition: Machine Translation System for India**, conducted at **IIT Kanpur**.  

**Goal:** Build **train-from-scratch Neural Machine Translation (NMT)** systems for **English â†’ Hindi (ENâ†’HI)** and **English â†’ Bengali (ENâ†’BN)** under strict constraints.

---

## ğŸ‘¤ Author

- **Name:** Sunny Raja Prasad  
- **Roll No:** 218171078  
- **Institute:** Indian Institute of Technology Kanpur  
- **Email:** sunnyrp21@iitk.ac.in  

---

## ğŸ“Œ Project Overview

The task involved building an **encoderâ€“decoder NMT system** using only the provided data, **without any pre-trained language models**, with **PyTorch-only implementations**.  

### Models Explored

- **Seq2Seq (no attention)**  
- **Seq2Seq + Attention**  
- **Transformer (trained from scratch)**  

âœ… The final submitted model was an **8-layer Convolutional Sequence-to-Sequence (ConvS2S) architecture with attention**.

---

## ğŸ† Competition Results

| Phase | Rank | chrF++ | ROUGE | BLEU |
|-------|------|--------|-------|------|
| Development | 31 | 0.385 | 0.404 | 0.138 |
| Test | 46 | 0.390 | 0.418 | 0.143 |

- **Codabench Username:** s_218171078  
- **Training Submissions:** 18  
- **Test Submissions:** 7  

---

## âš™ï¸ Key Features

- **Architecture:** Convolutional Seq2Seq (ConvS2S)  
- **Encoder / Decoder:** 8 convolutional layers each  
- **Embedding Dimension:** 512  
- **Hidden Dimension:** 512  
- **Kernel Size:** 3  
- **Attention:** Scaled dot-product attention at every decoder layer  
- **Decoding:** Batched greedy decoding  
- **Framework:** PyTorch (**from scratch**)  

---

## ğŸ§¹ Data & Preprocessing

- **Languages:** ENâ†’HI, ENâ†’BN  
- **Tokenization:** Word-level (`nltk.word_tokenize`)  
- **Vocabulary:** Minimum frequency = 2  
- **Special Tokens:** `<pad>`, `<sos>`, `<eos>`, `<unk>`  
- **Max Sequence Length:** 64  

### Text Cleaning Steps

- Lowercasing  
- Unicode normalization (NFC)  
- Removal of punctuation and digits  
- Whitespace normalization  

âš ï¸ Word-level tokenization caused a **large OOV (Out-of-Vocabulary) problem**, especially for **named entities**.

---

## ğŸ‹ï¸ Training Details

- **Loss:** Cross-Entropy (padding ignored)  
- **Optimizer:** Adam  
- **Learning Rate:** 5e-4  
- **LR Scheduler:** ReduceLROnPlateau  
- **Batch Size:** 32  
- **Dropout:** 0.25  
- **Gradient Clipping:** â€–gâ€–â‚‚ â‰¤ 1  
- **Teacher Forcing:** 1.0 (constant)  
- **Checkpointing:** Best validation model only  

---

## ğŸš€ Inference

- **Decoding Strategy:** Greedy search  
- **Stopping Criterion:** `<eos>` token or max length  
- **Detokenization:** Simple whitespace join  

âš ï¸ Greedy decoding was chosen for **speed**, but it introduces **repetition** and **early-stopping errors**.

---

## ğŸ” Error Analysis Summary

### Main Error Sources

- **OOV & Named Entities** â€” due to word-level vocabulary  
- **Missing digits & punctuation** â€” removed during preprocessing  
- **Repetitions & brevity** â€” caused by greedy decoding  
- **Long-distance dependencies** â€” limited by convolution kernel size  

---

## ğŸ“š Lessons Learned

- **Preprocessing choices are critical** â€” removing punctuation and digits significantly harms translation quality  
- **Word-level tokenization is a bottleneck** â€” it leads to massive **OOV issues**  
- **Greedy decoding trades quality for speed** â€” **beam search** would substantially improve results  

---

## ğŸ”® Future Improvements

- Replace word-level tokenization with **SentencePiece (BPE/Unigram)**  
- Implement **Beam Search** with length normalization  
- Preserve **punctuation and digits**  
- Explore **copy/transliteration mechanisms** for **named entities**  
- Compare optimized **ConvS2S** vs **BiLSTM + Attention** with **subwords**  

---

## ğŸ“– References

- Sutskever et al., *Sequence to Sequence Learning with Neural Networks*  
- Bahdanau et al., *Neural Machine Translation by Jointly Learning to Align and Translate*  
- Vaswani et al., *Attention Is All You Need*  
- Kunchukuttan & Bhattacharyya, *MT for Indian Language*  
